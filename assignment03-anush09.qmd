---
title: Assignment 03
author:
  - name: Anu Sharma
    affiliations:
      - id: anush09
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-09-21'
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
date-modified: today
date-format: long
execute:
  echo: false
  eval: true
  freeze: auto
---

```{python}
#| eval: true
#| echo: false
#| output: true
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when, lit
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id

np.random.seed(42)

pio.renderers.default = "notebook"

      # df = pd.read_csv("./data/lightcast_job_postings.csv")
      # unique_values = df['ONET_NAME'].unique()
      # print(unique_values)

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")
df.createOrReplaceTempView("job_postings")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

# df.printSchema() # comment this line when rendering the submission
# df.show(5)

# Section 1.1
df_cleaned = df \
    .withColumn("SALARY", col("SALARY").cast("integer")) \
    .withColumn("SALARY_FROM", col("SALARY_FROM").cast("integer")) \
    .withColumn("SALARY_TO", col("SALARY_TO").cast("integer")) \
    .withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast("integer")) \
    .withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast("integer"))

# df_cleaned = df_cleaned.withColumn("ONET_NAME", regexp_replace("ONET_NAME", "/", "-"))
df_cleaned.show(5)

# Section 1.2
from pyspark.sql import functions as sf
median_from = df_cleaned.approxQuantile("SALARY_FROM", [0.5], 0.0)[0]
median_to = df_cleaned.approxQuantile("SALARY_TO", [0.5], 0.0)[0]
median_salary = df_cleaned.approxQuantile("SALARY", [0.5], 0.0)[0]

print("Median From:", median_from)
print("Median To:", median_to)
print("Median Salary:", median_salary)

# Section 1.3
df_imputed = df_cleaned.withColumn(
    "SALARY",
    when(col("SALARY").isNull(), lit(median_salary)).otherwise(col("SALARY"))
)
df_imputed = df_imputed.withColumn(
    "SALARY_FROM",
    when(col("SALARY_FROM").isNull(), lit(median_from)).otherwise(col("SALARY_FROM"))
)
df_imputed = df_imputed.withColumn(
    "SALARY_TO",
    when(col("SALARY_TO").isNull(), lit(median_to)).otherwise(col("SALARY_TO"))
)

df_imputed = df_imputed.withColumn(
    "Average_Salary",
    ((col("SALARY_FROM") + col("SALARY_TO")) / 2)
)

# Section 1.4
from pyspark.sql.functions import regexp_replace
df_final = df_imputed.withColumn(
    "EDUCATION_LEVELS_NAME",
    regexp_replace("EDUCATION_LEVELS_NAME", "[\\n\\r]", "")
)

# Section 1.5
# df_cleaned.write.mode("overwrite").option("header", True).csv("data/cleaned_data.csv")
df_final.write.mode("overwrite").parquet("data/cleaned_data.parquet")
print(f"Data cleaning complete. Rows retained: {df_final.count()}")
df_final.createOrReplaceTempView("job_postings")
```


```{python}
#| eval: true
#| echo: true
#| output: true

# Section 2
salary_distribution = spark.sql("""
    SELECT 
        NAICS2_NAME,
        EMPLOYMENT_TYPE_NAME,
        SALARY_FROM
    FROM job_postings
    WHERE SALARY_FROM IS NOT NULL AND SALARY_FROM > 0
""")
salary_distribution.show()
df_pd = salary_distribution.toPandas()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))
sns.set_style("whitegrid")

sns.boxplot(
    data=df_pd,
    x="NAICS2_NAME",
    y="SALARY_FROM",
    hue="EMPLOYMENT_TYPE_NAME",
    palette="Set2"
)

plt.title("Salary Distribution by Industry", fontsize=16, fontweight="bold")
plt.xlabel("NAICS2_NAME", fontsize=12)
plt.ylabel("SALARY", fontsize=12)
plt.xticks(rotation=45)
plt.legend(title="Employment Type", fontsize=10)
plt.tight_layout()
plt.show()

```

```{python}
#| eval: true
#| echo: true
#| output: true

# Section 3
# df_bubble = spark.sql("""
#     SELECT
#         LOT_OCCUPATION_NAME,
#         percentile_approx(SALARY, 0.5) AS MEDIAN_SALARY,
#         COUNT(*) AS JOB_COUNT
#     FROM job_postings
#     WHERE SALARY IS NOT NULL AND SALARY > 0 AND LOT_OCCUPATION_NAME IS NOT NULL
#     GROUP BY LOT_OCCUPATION_NAME
# """).toPandas()
# import matplotlib.pyplot as plt

# plt.figure(figsize=(16, 12))
# scatter = plt.scatter(
#     x=df_bubble["LOT_OCCUPATION_NAME"],
#     y=df_bubble["MEDIAN_SALARY"],
#     s=df_bubble["JOB_COUNT"],
#     c=df_bubble["MEDIAN_SALARY"],
#     cmap="coolwarm",
#     alpha=0.7,
#     edgecolors="black",
#     linewidth=0.5
# )

# plt.xticks(rotation=45, ha="right")
# plt.title("Median Salary by ONET Occupation Type", fontsize=18, fontweight="bold")
# plt.xlabel("ONET Occupation", fontsize=14)
# plt.ylabel("Median Salary", fontsize=14)
# plt.grid(True)
# plt.colorbar(scatter, label="Median Salary")
# plt.tight_layout()
# plt.show()


# Bubble Chart: Median Salary by LOT_OCCUPATION_NAME and Job Count
bubble_df = spark.sql("""
    SELECT
        LOT_OCCUPATION_NAME,
        percentile_approx(SALARY, 0.5) AS MEDIAN_SALARY,
        COUNT(*) AS JOB_COUNT
    FROM job_postings
    WHERE SALARY IS NOT NULL AND SALARY > 0 AND LOT_OCCUPATION_NAME IS NOT NULL
    GROUP BY LOT_OCCUPATION_NAME
    ORDER BY JOB_COUNT DESC
""").toPandas()

print("Top 5 occupations by job count and median salary:")
print(bubble_df.head())

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 10))
scatter = plt.scatter(
    x=bubble_df["LOT_OCCUPATION_NAME"],
    y=bubble_df["MEDIAN_SALARY"],
    s=bubble_df["JOB_COUNT"] / 10,  # Scale bubble size
    c=bubble_df["MEDIAN_SALARY"],
    cmap="viridis",
    alpha=0.7,
    edgecolors="black",
    linewidth=0.5
)

plt.xticks(rotation=60, ha="right")
plt.title("Median Salary by Occupation (Bubble size = Job Count)", fontsize=16, fontweight="bold")
plt.xlabel("Occupation Name")
plt.ylabel("Median Salary")
plt.colorbar(scatter, label="Median Salary")
plt.tight_layout()
plt.show()

```

```{python}
#| eval: true
#| echo: true
#| output: true

from pyspark.sql.functions import col, when
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np


df_grouped = df.withColumn("EDUCATION_GROUP",
    when(col("MIN_EDULEVELS_NAME").rlike("(?i)ged|associate|no education"), "Associate or Lower")
    .when(col("MIN_EDULEVELS_NAME").rlike("(?i)bachelor"), "Bachelor's")
    .when(col("MIN_EDULEVELS_NAME").rlike("(?i)master"), "Master's")
    .when(col("MIN_EDULEVELS_NAME").rlike("(?i)ph\\.d|doctorate|professional"), "PhD")
)


df_grouped = df_grouped.filter(
    col("EDUCATION_GROUP").isNotNull() &
    col("SALARY").isNotNull() &
    col("MAX_YEARS_EXPERIENCE").isNotNull()
)


df_grouped.createOrReplaceTempView("education_jobs")


scatter_data = spark.sql("""
    SELECT 
        EDUCATION_GROUP,
        LOT_V6_SPECIALIZED_OCCUPATION_NAME,
        MAX_YEARS_EXPERIENCE,
        AVG(SALARY) AS AVERAGE_SALARY
    FROM education_jobs
    GROUP BY EDUCATION_GROUP, LOT_V6_SPECIALIZED_OCCUPATION_NAME, MAX_YEARS_EXPERIENCE
""")

# Scatter plit
df_plot = scatter_data.toPandas()
df_plot["JITTER_EXPERIENCE"] = df_plot["MAX_YEARS_EXPERIENCE"] + np.random.normal(0, 0.5, size=len(df_plot))

plt.figure(figsize=(14, 12))
sns.set_style("whitegrid")

sns.scatterplot(
    data=df_plot,
    x="JITTER_EXPERIENCE",
    y="AVERAGE_SALARY",
    hue="EDUCATION_GROUP",
    style="EDUCATION_GROUP",
    palette="Set2",
    alpha=0.7
)

plt.title("Salary vs Experience by Education Level", fontsize=16, fontweight="bold")
plt.xlabel("Max Years Experience (with jitter)", fontsize=12)
plt.ylabel("Average Salary", fontsize=12)
min_exp = int(df_plot["JITTER_EXPERIENCE"].min())
max_exp = int(df_plot["JITTER_EXPERIENCE"].max()) + 1
plt.xticks(range(min_exp, max_exp, 1))
plt.legend(title="Education Level", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()


```

```{python}
#| eval: true
#| echo: true
#| output: true

from pyspark.sql.functions import when, col, trim, regexp_replace
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Create REMOTE_GROUP column
df_remote = spark.sql("SELECT * FROM job_postings")
spark.sql("""
    SELECT DISTINCT REMOTE_TYPE_NAME
    FROM job_postings
    WHERE REMOTE_TYPE_NAME IS NOT NULL
""").show(truncate=False)


from pyspark.sql.functions import lower, trim, col, when
df_remote = df_remote.withColumn("REMOTE_TYPE_NAME_CLEAN", trim(lower(col("REMOTE_TYPE_NAME"))))
df_remote = df_remote.withColumn("REMOTE_GROUP",
    when(col("REMOTE_TYPE_NAME_CLEAN") == "not remote", "Onsite")
    .when(col("REMOTE_TYPE_NAME_CLEAN").contains("hybrid"), "Hybrid")
    .when(col("REMOTE_TYPE_NAME_CLEAN").contains("remote"), "Remote")
    .otherwise("Onsite")
)

# Filter and create temp view
df_remote_filtered = df_remote.filter(
    col("SALARY").isNotNull() &
    col("MAX_YEARS_EXPERIENCE").isNotNull() &
    col("LOT_V6_SPECIALIZED_OCCUPATION_NAME").isNotNull()
)
df_remote_filtered.createOrReplaceTempView("remote_jobs")

scatter_data = spark.sql("""
    SELECT 
        REMOTE_GROUP,
        LOT_V6_SPECIALIZED_OCCUPATION_NAME,
        MAX_YEARS_EXPERIENCE,
        AVG(SALARY) AS AVERAGE_SALARY
    FROM remote_jobs
    GROUP BY REMOTE_GROUP, LOT_V6_SPECIALIZED_OCCUPATION_NAME, MAX_YEARS_EXPERIENCE
""")

# Scatter plot
df_plot = scatter_data.toPandas()
df_plot["JITTER_EXPERIENCE"] = df_plot["MAX_YEARS_EXPERIENCE"] #+ np.random.normal(0, 0.5, size=len(df_plot))

plt.figure(figsize=(14, 12))
sns.set_style("whitegrid")

sns.scatterplot(
    data=df_plot,
    x="JITTER_EXPERIENCE",
    y="AVERAGE_SALARY",
    hue="REMOTE_GROUP",
    style="REMOTE_GROUP",
    palette="Set2",
    alpha=0.7
)

plt.title("Salary vs Experience by Remote Work Type", fontsize=16, fontweight="bold")
plt.xlabel("Max Years Experience (with jitter)", fontsize=12)
plt.ylabel("Average Salary", fontsize=12)
plt.legend(title="Remote Work Type", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Histogram
df_hist = df_remote_filtered.select("SALARY", "REMOTE_GROUP").toPandas()

plt.figure(figsize=(12, 7))
sns.histplot(data=df_hist, x="SALARY", hue="REMOTE_GROUP", bins=40, kde=True, palette="Set2", alpha=0.6)
plt.title("Salary Distribution by Remote Work Type", fontsize=16, fontweight="bold")
plt.xlabel("Salary", fontsize=12)
plt.ylabel("Frequency", fontsize=12)
plt.tight_layout()
plt.show()


```
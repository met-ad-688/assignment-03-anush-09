---
title: Assignment 03
author:
  - name: Anu Sharma
    affiliations:
      - id: anush09
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-09-21'
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
date-modified: today
date-format: long
execute:
  echo: false
  eval: true
  freeze: auto
---

```{python}
#| eval: true
#| echo: false
#| output: true
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id

np.random.seed(42)

pio.renderers.default = "notebook"

      # df = pd.read_csv("./data/lightcast_job_postings.csv")
      # unique_values = df['ONET_NAME'].unique()
      # print(unique_values)

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")
df.createOrReplaceTempView("job_postings")

# Show Schema and Sample Data
print("---This is Diagnostic check, No need to print it in the final doc---")

df.printSchema() # comment this line when rendering the submission
df.show(5)

from pyspark.sql.functions import col


from pyspark.sql.functions import col
# Section 1.1
df_cleaned = df \
    .withColumn("SALARY", col("SALARY").cast("integer")) \
    .withColumn("SALARY_FROM", col("SALARY_FROM").cast("integer")) \
    .withColumn("SALARY_TO", col("SALARY_TO").cast("integer")) \
    .withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast("integer")) \
    .withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast("integer"))

df_cleaned = df_cleaned.withColumn("ONET_NAME", regexp_replace("ONET_NAME", "/", "-"))
df_cleaned.show(5)

# Section 1.2
from pyspark.sql import functions as sf
median_from = df_cleaned.approxQuantile("SALARY_FROM", [0.5], 0.0)[0]
median_to = df_cleaned.approxQuantile("SALARY_TO", [0.5], 0.0)[0]
median_salary = df_cleaned.approxQuantile("SALARY", [0.5], 0.0)[0]

print("Median From:", median_from)
print("Median To:", median_to)
print("Median Salary:", median_salary)

# Section 1.3
from pyspark.sql.functions import when, lit
df_imputed = df_cleaned.withColumn(
    "SALARY",
    when(col("SALARY").isNull(), lit(median_salary)).otherwise(col("SALARY"))
)

# Section 1.4
from pyspark.sql.functions import regexp_replace
df_cleaned = df_cleaned.withColumn(
    "EDUCATION_LEVELS_NAME",
    regexp_replace("EDUCATION_LEVELS_NAME", "[\\n\\r]", "")
)

# Section 1.5
# df_cleaned.write.mode("overwrite").option("header", True).csv("data/cleaned_data.csv")
df_cleaned.write.mode("overwrite").parquet("data/cleaned_data.parquet")
print(f"Data cleaning complete. Rows retained: {df_cleaned.count()}")
df_cleaned.createOrReplaceTempView("job_postings")
```

```{python}
#| eval: false
#| echo: true
#| output: true

# Read the cleaned CSV data
import pandas as pd
import plotly.express as px
import plotly.io as pio
from pyspark.sql import SparkSession
import re
import numpy as np
import plotly.graph_objects as go
from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when
from pyspark.sql import functions as F
from pyspark.sql.functions import col, monotonically_increasing_id

np.random.seed(42)

pio.renderers.default = "notebook"
# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.parquet("data/cleaned_data.parquet")
df.createOrReplaceTempView("job_postings")
df.show(5)

```

```{python}
#| eval: true
#| echo: true
#| output: true

# Section 2
salary_distribution = spark.sql("""
    SELECT 
        NAICS2_NAME,
        EMPLOYMENT_TYPE_NAME,
        SALARY_FROM
    FROM job_postings
    WHERE SALARY_FROM IS NOT NULL AND SALARY_FROM > 0
""")
salary_distribution.show()
df_pd = salary_distribution.toPandas()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(18, 12))
sns.set_style("whitegrid")

sns.boxplot(
    data=df_pd,
    x="NAICS2_NAME",
    y="SALARY_FROM",
    hue="EMPLOYMENT_TYPE_NAME",
    palette="Set2"
)

plt.title("Salary Distribution by Industry", fontsize=16, fontweight="bold")
plt.xlabel("NAICS2_NAME", fontsize=12)
plt.ylabel("SALARY", fontsize=12)
plt.xticks(rotation=45)
plt.legend(title="Employment Type", fontsize=10)
plt.tight_layout()
plt.show()

```

```{python}
#| eval: true
#| echo: true
#| output: true

# Section 3
salary_bubble = spark.sql("""
    SELECT 
        LOT_OCCUPATION_NAME,
        approx_percentile(SALARY, 0.5) AS MEDIAN_SALARY,
        COUNT(*) AS JOB_COUNT
    FROM job_postings
    WHERE SALARY IS NOT NULL AND SALARY > 0
    GROUP BY LOT_OCCUPATION_NAME
""")

df_bubble = salary_bubble.toPandas()

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 12))
scatter = plt.scatter(
    x=df_bubble["LOT_OCCUPATION_NAME"],
    y=df_bubble["MEDIAN_SALARY"],
    s=df_bubble["JOB_COUNT"],
    c=df_bubble["MEDIAN_SALARY"],
    cmap="coolwarm",
    alpha=0.7,
    edgecolors="black",
    linewidth=0.5
)

plt.xticks(rotation=45, ha="right")
plt.title("Median Salary by ONET Occupation Type", fontsize=18, fontweight="bold")
plt.xlabel("ONET Occupation", fontsize=14)
plt.ylabel("Median Salary", fontsize=14)
plt.grid(True)
plt.colorbar(scatter, label="Median Salary")
plt.tight_layout()
plt.show()

```

```{python}
#| eval: true
#| echo: true
#| output: true
#| 
from pyspark.sql.functions import split, explode, trim, lower, col, regexp_replace, when
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Parse EDUCATION_LEVELS_NAME into individual entries
df_parsed = spark.sql("""
    SELECT *,
        split(regexp_replace(regexp_replace(EDUCATION_LEVELS_NAME, '^\[|\\]$', ''), '\"', ''), ',') AS EDUCATION_ARRAY
    FROM job_postings
""")

#  Explode and normalize
df_exploded = df_parsed.withColumn("EDUCATION_ENTRY", explode("EDUCATION_ARRAY"))
df_exploded = df_exploded.withColumn("EDUCATION_ENTRY_CLEAN", trim(lower(col("EDUCATION_ENTRY"))))

df_grouped = df_exploded.withColumn("EDUCATION_GROUP",
    when(col("EDUCATION_ENTRY_CLEAN").isin("ged", "associate degree", "no education listed", "high school or ged"), "Associate or Lower")
    .when(col("EDUCATION_ENTRY_CLEAN") == "bachelor's degree", "Bachelor's")
    .when(col("EDUCATION_ENTRY_CLEAN") == "master's degree", "Master's")
    .when(col("EDUCATION_ENTRY_CLEAN") == "ph.d. or professional degree", "PhD")
)

df_grouped = df_grouped.filter(
    col("EDUCATION_GROUP").isNotNull() &
    col("SALARY").isNotNull() &
    col("MAX_YEARS_EXPERIENCE").isNotNull()
)
df_grouped.createOrReplaceTempView("education_jobs")

scatter_data = spark.sql("""
    SELECT 
        EDUCATION_GROUP,
        LOT_V6_SPECIALIZED_OCCUPATION_NAME,
        MAX_YEARS_EXPERIENCE,
        AVG(SALARY) AS AVERAGE_SALARY
    FROM education_jobs
    GROUP BY EDUCATION_GROUP, LOT_V6_SPECIALIZED_OCCUPATION_NAME, MAX_YEARS_EXPERIENCE
""")

# Scatter plit
df_plot = scatter_data.toPandas()
df_plot["JITTER_EXPERIENCE"] = df_plot["MAX_YEARS_EXPERIENCE"] #+ np.random.normal(0, 0.5, size=len(df_plot))

plt.figure(figsize=(14, 12))
sns.set_style("whitegrid")

sns.scatterplot(
    data=df_plot,
    x="JITTER_EXPERIENCE",
    y="AVERAGE_SALARY",
    hue="EDUCATION_GROUP",
    style="EDUCATION_GROUP",
    palette="Set2",
    alpha=0.7
)

plt.title("Salary vs Experience by Education Level", fontsize=16, fontweight="bold")
plt.xlabel("Max Years Experience (with jitter)", fontsize=12)
plt.ylabel("Average Salary", fontsize=12)
plt.legend(title="Education Level", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()


```

```{python}
#| eval: true
#| echo: true
#| output: true

from pyspark.sql.functions import when, col, trim, regexp_replace
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Create REMOTE_GROUP column
df_remote = spark.sql("SELECT * FROM job_postings")
spark.sql("""
    SELECT DISTINCT REMOTE_TYPE_NAME
    FROM job_postings
    WHERE REMOTE_TYPE_NAME IS NOT NULL
""").show(truncate=False)


from pyspark.sql.functions import lower, trim, col, when
df_remote = df_remote.withColumn("REMOTE_TYPE_NAME_CLEAN", trim(lower(col("REMOTE_TYPE_NAME"))))
df_remote = df_remote.withColumn("REMOTE_GROUP",
    when(col("REMOTE_TYPE_NAME_CLEAN") == "not remote", "Onsite")
    .when(col("REMOTE_TYPE_NAME_CLEAN").contains("hybrid"), "Hybrid")
    .when(col("REMOTE_TYPE_NAME_CLEAN").contains("remote"), "Remote")
    .otherwise("Onsite")
)

# Filter and create temp view
df_remote_filtered = df_remote.filter(
    col("SALARY").isNotNull() &
    col("MAX_YEARS_EXPERIENCE").isNotNull() &
    col("LOT_V6_SPECIALIZED_OCCUPATION_NAME").isNotNull()
)
df_remote_filtered.createOrReplaceTempView("remote_jobs")

scatter_data = spark.sql("""
    SELECT 
        REMOTE_GROUP,
        LOT_V6_SPECIALIZED_OCCUPATION_NAME,
        MAX_YEARS_EXPERIENCE,
        AVG(SALARY) AS AVERAGE_SALARY
    FROM remote_jobs
    GROUP BY REMOTE_GROUP, LOT_V6_SPECIALIZED_OCCUPATION_NAME, MAX_YEARS_EXPERIENCE
""")

# Scatter plot
df_plot = scatter_data.toPandas()
df_plot["JITTER_EXPERIENCE"] = df_plot["MAX_YEARS_EXPERIENCE"] #+ np.random.normal(0, 0.5, size=len(df_plot))

plt.figure(figsize=(14, 12))
sns.set_style("whitegrid")

sns.scatterplot(
    data=df_plot,
    x="JITTER_EXPERIENCE",
    y="AVERAGE_SALARY",
    hue="REMOTE_GROUP",
    style="REMOTE_GROUP",
    palette="Set2",
    alpha=0.7
)

plt.title("Salary vs Experience by Remote Work Type", fontsize=16, fontweight="bold")
plt.xlabel("Max Years Experience (with jitter)", fontsize=12)
plt.ylabel("Average Salary", fontsize=12)
plt.legend(title="Remote Work Type", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Histogram
df_hist = df_remote_filtered.select("SALARY", "REMOTE_GROUP").toPandas()

plt.figure(figsize=(12, 7))
sns.histplot(data=df_hist, x="SALARY", hue="REMOTE_GROUP", bins=40, kde=True, palette="Set2", alpha=0.6)
plt.title("Salary Distribution by Remote Work Type", fontsize=16, fontweight="bold")
plt.xlabel("Salary", fontsize=12)
plt.ylabel("Frequency", fontsize=12)
plt.tight_layout()
plt.show()


```